import subprocess
import os
import sys
import yaml
import datetime
import torch # Need torch to check for GPU

def check_gpu():
    """Checks for GPU availability and prints status"""
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        print(f"\nğŸ”¥ GPU DETECTED: {gpu_name}")
        print("   Training will be accelerated.")
        return True
    else:
        print("\nâš ï¸ NO GPU DETECTED. Running in CPU mode.")
        print("   Training might be slow.")
        return False

def run_command(command, step_name, cwd=None):
    """Helper to run shell commands and handle errors"""
    print(f"\n{'='*50}")
    print(f"ğŸš€ STARTING: {step_name}")
    print(f"{'='*50}\n")
    try:
        # Run the command and wait for it to finish. 
        subprocess.run(command, check=True, shell=True, cwd=cwd)
        print(f"\nâœ… COMPLETED: {step_name}")
    except subprocess.CalledProcessError as e:
        print(f"\nâŒ FAILED: {step_name}")
        print(f"Error details: {e}")
        if "Docker" not in step_name:
             sys.exit(1)

def generate_report(gpu_active):
    """Reads config and results to generate a Markdown report"""
    print("\nğŸ“ Generating REPORT.md...")
    
    with open('config.yaml', 'r') as f:
        config = yaml.safe_load(f)
    
    if not os.path.exists('results.yaml'):
        results = {'best_val_loss': 'N/A', 'final_accuracy': 0, 'final_f1_score': 0}
    else:
        with open('results.yaml', 'r') as f:
            results = yaml.safe_load(f)

    # Determine correct docker command based on GPU availability
    docker_cmd = "docker run --gpus all -p 8000:8000 emotitune-api" if gpu_active else "docker run -p 8000:8000 emotitune-api"

    md_content = f"""# ğŸµ Emotitune Experiment Report
**Date:** {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## ğŸ–¥ï¸ Hardware
* **Device Used:** `{'GPU ğŸš€' if gpu_active else 'CPU ğŸŒ'}`

## ğŸ“Š Summary of Results
| Metric | Value |
| :--- | :--- |
| **Best Validation Loss** | `{results['best_val_loss']}` |
| **Final Accuracy** | `{results['final_accuracy']*100:.2f}%` |
| **Weighted F1-Score** | `{results['final_f1_score']:.4f}` |

## âš™ï¸ Configuration
* **Epochs:** {config['training']['num_epochs']}
* **Batch Size:** {config['training']['batch_size']}
* **Learning Rate:** {config['training']['learning_rate']}

## ğŸ³ Deployment
Run the containerized API with:
```bash
{docker_cmd}

Report automatically generated by run.py 
""" 
    with open('REPORT.md', 'w') as f: 
        f.write(md_content) 
        print("âœ¨ REPORT.md created successfully!")

def main(): 
    python_executable = sys.executable

# 0. Check Hardware
    gpu_active = check_gpu()

# 1. Feature Extraction
    run_command(f"{python_executable} src/audio/preprocess.py", "Parallel Feature Extraction")

# 2. Training
    run_command(f"{python_executable} src/audio/cnn_baseline.py", "Model Training")

# 3. Docker Build
    run_command("docker build -t emotitune-api .", "Docker Build")

# 4. Generate Report
    generate_report(gpu_active)

    print("\nğŸ‰ PIPELINE FINISHED! Check REPORT.md for details.")

if __name__ == "__main__": 
    main()